name: Sync Podcast Index Data

on:
  schedule:
    # Run every 6 hours at minute 30 (avoids collision with boxcast-api cron)
    - cron: '30 */6 * * *'
  workflow_dispatch: # Allow manual trigger

jobs:
  sync-pi-data:
    runs-on: ubuntu-latest
    timeout-minutes: 120
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Download Podcast Index Database
        run: |
          echo "Downloading PI database dump..."
          curl -L -o podcastindex_feeds.db.tgz https://public.podcastindex.org/podcastindex_feeds.db.tgz
          echo "Extracting..."
          tar -xzf podcastindex_feeds.db.tgz
          ls -la *.db
          
      - name: Inspect PI dump schema
        run: |
          echo "=== Tables in PI dump ==="
          sqlite3 podcastindex_feeds.db ".tables"
          echo ""
          echo "=== Podcasts/Feeds table schema ==="
          sqlite3 podcastindex_feeds.db ".schema podcasts" || sqlite3 podcastindex_feeds.db ".schema newsfeeds" || true
          echo ""
          echo "=== Sample row ==="
          sqlite3 podcastindex_feeds.db "SELECT * FROM podcasts LIMIT 1;" 2>/dev/null || sqlite3 podcastindex_feeds.db "SELECT * FROM newsfeeds LIMIT 1;" || true
          echo ""
          echo "=== Episodes table schema ==="
          sqlite3 podcastindex_feeds.db ".schema episodes" || true
          
      - name: Get chart iTunes IDs
        env:
          TURSO_URL: ${{ secrets.TURSO_URL }}
          TURSO_AUTH_TOKEN: ${{ secrets.TURSO_AUTH_TOKEN }}
        run: |
          node scripts/get-chart-itunes-ids.js > chart_itunes_ids.txt
          echo "Got $(wc -l < chart_itunes_ids.txt) iTunes IDs from charts"
          head -5 chart_itunes_ids.txt
          
      - name: Export podcasts from PI dump
        run: |
          # Create temp table with iTunes IDs
          sqlite3 podcastindex_feeds.db "CREATE TABLE chart_ids (itunes_id INTEGER);"
          sqlite3 -csv podcastindex_feeds.db ".import chart_itunes_ids.txt chart_ids"
          echo "Imported $(sqlite3 podcastindex_feeds.db 'SELECT COUNT(*) FROM chart_ids;') iTunes IDs"
          
          # Export podcasts - using correct column names from PI dump
          echo "Exporting podcasts..."
          sqlite3 -header -csv podcastindex_feeds.db "
            SELECT 
              p.id,
              p.itunesId as itunes_id,
              p.title,
              p.itunesAuthor as author,
              p.description,
              p.imageUrl as image_url,
              p.url as feed_url,
              p.link as website_url,
              p.category1 || ',' || p.category2 as categories,
              p.language,
              p.explicit,
              p.itunesType as type,
              p.episodeCount,
              p.newestItemPubdate,
              p.lastUpdate
            FROM podcasts p
            WHERE p.itunesId IN (SELECT itunes_id FROM chart_ids);
          " > podcasts_export.csv
          echo "Exported $(wc -l < podcasts_export.csv) podcasts"
          


      - name: Import to Turso
        env:
          TURSO_URL: ${{ secrets.TURSO_URL }}
          TURSO_AUTH_TOKEN: ${{ secrets.TURSO_AUTH_TOKEN }}
        run: node scripts/import-pi-data.js

      - name: Sync Episodes via API
        env:
          TURSO_URL: ${{ secrets.TURSO_URL }}
          TURSO_AUTH_TOKEN: ${{ secrets.TURSO_AUTH_TOKEN }}
          PODCAST_INDEX_API_KEY: ${{ secrets.PODCAST_INDEX_API_KEY }}
          PODCAST_INDEX_API_SECRET: ${{ secrets.PODCAST_INDEX_API_SECRET }}
        run: node scripts/sync-episodes.js
          
      - name: Cleanup
        run: rm -f *.db *.tgz *.csv *.txt
